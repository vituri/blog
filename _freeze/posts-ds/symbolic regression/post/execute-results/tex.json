{
  "hash": "fb605be948fc3f494c6dbc96984a949d",
  "result": {
    "engine": "julia",
    "markdown": "---\nengine: julia\n\njulia:\n    exeflags: [\"--project=@v#.#\"]\n\ntitle: \"Symbolic Regression in Julia\"\ndate: 2024-03-14\n\nformat: \n    pdf: default\n    html: default\n---\n\n\n\n\n![](cover.png)\n\n\n## What is it?\n\nA linear regression finds the line that is \"closest\" to a dataset. In a similar maner, a symbolic regression is an algorithm that find a combination of symbols that minimizes the mean square error of a given dataset. These symbols are unary and binary operators like the + symbol or a function like $cos$ and $1/x$.\n\n## Example 1\n\nLet's try to approximate the function $f(x) = - x^2 + 1$ using the symbols and $+, -, *$ combined with the variable $x$. \n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\n# import Pkg; \n# Pkg.add([\"SymbolicRegression\", \"MLJ\", \"SymbolicUtils\", \"Plots\"])\n```\n:::\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nusing SymbolicRegression, MLJ, SymbolicUtils\nusing Plots\n\nx = [-3:0.1:3;]\ny = @. - x^2 + 1;\n\nscatter(x, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](post_files/figure-pdf/cell-3-output-1.svg){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nFirst we define a model\n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nmodel = SRRegressor(\n    binary_operators=[+, -, *],    \n    niterations=50,\n    seed = 1\n);\n```\n:::\n\n\n\n\n\n\n(Note: the argument `seed = 1` is needed to ensure that the result is the same when this Quarto document compiles; you don't need it.)\n\nAnd then fit it to our dataset\n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nX = reshape(x, (length(x), 1))\n\nmach = machine(model, X, y)\nfit!(mach)\n```\n:::\n\n\n\n\n\n\nWe can see a report about the results:\n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nr = report(mach);\n\nr\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n(best_idx = 2,\n equations = DynamicExpressions.EquationModule.Node{Float64}[-2.1000000001717973, 1.0 - (x₁ * x₁)],\n equation_strings = [\"-2.1000000001717973\", \"1.0 - (x₁ * x₁)\"],\n losses = [7.681799999999998, 0.0],\n complexities = [1, 5],\n scores = [36.04365338911715, 9.010913347279288],)\n```\n:::\n:::\n\n\n\n\n\n\nThis report contains the losses\n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nr.losses\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n2-element Vector{Float64}:\n 7.681799999999998\n 0.0\n```\n:::\n:::\n\n\n\n\n\n\nthe equations\n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nr.equations\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n2-element Vector{DynamicExpressions.EquationModule.Node{Float64}}:\n -2.1000000001717973\n 1.0 - (x₁ * x₁)\n```\n:::\n:::\n\n\n\n\n\n\nand the best one of the functions found (ie. the one with the least loss):\n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nnode_to_symbolic(r.equations[r.best_idx], model)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n1.0 - (x1 * x1)\n```\n:::\n:::\n\n\n\n\n\n\nHere, we can read $x_1$ as $x$, because we only have one variable.\n\nNotice that this expression simplifies to our original $f$.\n\n## Example 2\n\nNow let's get a more interesting example. Take $f(x) = x^2 + 2cos(x)^2$:\n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\ny = @. x^2 + 2cos(x)^2 \n\nscatter(x, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](post_files/figure-pdf/cell-10-output-1.svg){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nWe again create a model and fit it, but now we allow more operations: besides the earlier binary functions, we also have the unary `cos` function:\n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nmodel = SRRegressor(\n    binary_operators = [+, -, *],    \n    unary_operators = [cos],\n    niterations=50,\n    seed = 1\n);\n\nmach = machine(model, X, y)\nfit!(mach)\n```\n:::\n\n\n\n\n\n\nand see the best equation:\n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nr = report(mach)\nnode_to_symbolic(r.equations[r.best_idx], model)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\ncos(x1 + x1) + (cos(x1 - x1) + (x1 * x1))\n```\n:::\n:::\n\n\n\n\n\n\nSo, we got\n\n$$\nx * x + cos(x + x) - (-1) = x^2 + cos(2x) + 1\n$$\n\nSince $cos(2x) + 1 = 2cos^2(x)$, we retrieve the original function.\n\n## Example 3\n\nEven after adding some noise to the original dataset, the symbolic regression still can find a very good approximation:\n\nTake $f(x) = 0.3 * x^3 - x^2 + 2cos(x) + \\epsilon(x)$ where $\\epsilon(x)$ is a random uniform error (varying in $[0, 1]$) like this:\n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nx = [-5:0.1:5;]\nX = reshape(x, (length(x), 1))\nerrors = rand(length(x))\ny = @. 0.3*x^3 - x^2 + 2cos(x) + errors\n\nscatter(x, y)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](post_files/figure-pdf/cell-13-output-1.svg){fig-pos='H'}\n:::\n:::\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nmodel = SRRegressor(\n    binary_operators = [+, -, *],    \n    unary_operators = [cos],\n    niterations=60,\n    seed = 1\n);\n\nmach = machine(model, X, y)\nfit!(mach)\n```\n:::\n\n\n\n\n\n\nand see the best equation:\n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\nr = report(mach)\nnode_to_symbolic(r.equations[r.best_idx], model)\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```\n((cos(x1) * 1.9406108060970928) - ((x1 * ((x1 + -3.343969169013866) * x1)) * -0.300261543749164)) + 0.5269577542465653\n```\n:::\n:::\n\n\n\n\n\n\nWe can plot the prediction and the original dataset to compare them:\n\n\n\n\n::: {.cell execution_count=1}\n``` {.julia .cell-code}\ny_pred = predict(mach, X)\n \nscatter(x, y);\nscatter!(x, y_pred, color = \"red\")\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n![](post_files/figure-pdf/cell-16-output-1.svg){fig-pos='H'}\n:::\n:::\n\n\n\n\n\n\nNot bad at all!\n\nYou can see more about this package in [this link](https://astroautomata.com/SymbolicRegression.jl/dev/examples/). If you have enough courage, read the [original paper](https://arxiv.org/abs/2305.01582) on arxiv!\n\n",
    "supporting": [
      "post_files"
    ],
    "filters": []
  }
}